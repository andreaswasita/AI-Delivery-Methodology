# Risk Library for AI Projects
## Shared Component: 18 Pre-Defined Risk Templates

**Purpose**: Central library of common AI project risks with detailed mitigation strategies  
**Use In**: Risk registers, business cases, phase planning, executive briefings  
**Version**: 1.0  
**Last Updated**: January 13, 2026

---

## Quick Links

- ðŸ“Š **Use This In**: [Risk Register Template](../05-risk-register.md)
- ðŸ“– **Related**: [ROI Analysis](./roi-financial-analysis.md) | [Stakeholder Personas](./stakeholder-personas.md)
- ðŸŽ¯ **Quick Reference**: [Risk Quick Reference (1-page)](./risk-quick-reference.md)

---

## How to Use This Library

### For New Projects:
1. Review all 18 pre-defined risks
2. Select relevant risks for your project context
3. Copy selected risks into your [Risk Register](../05-risk-register.md)
4. Customize probability, impact, and mitigation plans
5. Add project-specific risks

### Risk Scoring:
- **Probability**: 1 (Very Low) to 5 (Very High)
- **Impact**: 1 (Very Low) to 5 (Critical)
- **Risk Score**: Probability Ã— Impact
- **Priority**: 15-25 (Critical), 10-14 (High), 5-9 (Medium), 1-4 (Low)

---

## CATEGORY 1: TECHNICAL RISKS (5 risks)

### R001: Inadequate Data Quality

| Field | Details |
|-------|---------|
| **Risk ID** | R001 |
| **Category** | Technical - Data |
| **Description** | Available data may have poor quality, missing values, or inconsistencies that prevent effective model training |
| **Typical Causes** | Legacy systems, data entry errors, incomplete data collection processes, poor data governance |
| **Typical Impact** | Model accuracy below acceptable thresholds, project delays, additional data cleansing effort required |
| **Typical Probability** | 4 - High (occurs in 60-80% of AI projects) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 16 (High Priority) |
| **Mitigation Strategy** | â€¢ Conduct comprehensive data quality assessment early in discovery phase<br>â€¢ Implement automated data validation rules<br>â€¢ Set up continuous data quality monitoring<br>â€¢ Allocate 30-40% of project time for data preparation<br>â€¢ Engage data stewards from source systems<br>â€¢ Create data quality SLAs with data owners |
| **Contingency Plan** | â€¢ Engage data stewards to improve source data quality<br>â€¢ Consider synthetic data generation techniques<br>â€¢ Adjust model approach to handle data limitations<br>â€¢ Use data imputation and cleaning algorithms<br>â€¢ Scope model to high-quality data subset |
| **Trigger Indicators** | â€¢ Data completeness < 80%<br>â€¢ High error rates in validation (>10%)<br>â€¢ Significant outliers (>3 standard deviations)<br>â€¢ Inconsistent data formats across sources<br>â€¢ Missing critical fields |
| **Best Practices** | â€¢ Use 6-dimension data quality framework (Accuracy, Completeness, Consistency, Timeliness, Validity, Uniqueness)<br>â€¢ Create data quality dashboards<br>â€¢ Implement data lineage tracking |

---

### R002: Model Performance Below Expectations

| Field | Details |
|-------|---------|
| **Risk ID** | R002 |
| **Category** | Technical - AI/ML |
| **Description** | ML model fails to achieve target accuracy, precision, or recall metrics defined in success criteria |
| **Typical Causes** | Insufficient training data, poor feature selection, algorithm limitations, concept drift, unrealistic expectations |
| **Typical Impact** | Business value not realized, stakeholder confidence lost, project cancellation risk |
| **Typical Probability** | 3 - Medium (30-40% of AI projects) |
| **Typical Impact** | 5 - Critical |
| **Typical Risk Score** | 15 (High Priority) |
| **Mitigation Strategy** | â€¢ Set realistic performance targets based on baseline and industry benchmarks<br>â€¢ Use ensemble methods and state-of-the-art algorithms<br>â€¢ Implement comprehensive feature engineering<br>â€¢ Plan for 3-5 modeling iterations<br>â€¢ Engage external ML experts for review<br>â€¢ Use transfer learning from pre-trained models<br>â€¢ Conduct thorough exploratory data analysis |
| **Contingency Plan** | â€¢ Adjust success criteria if business value still achieved<br>â€¢ Pivot to hybrid approach (rules + AI)<br>â€¢ Implement human-in-the-loop workflow<br>â€¢ Scope model to high-confidence predictions only<br>â€¢ Use explainable AI to identify failure modes |
| **Trigger Indicators** | â€¢ Prototype model < 70% of target performance<br>â€¢ No improvement after 3 modeling iterations<br>â€¢ Cross-validation results declining<br>â€¢ Validation loss increasing<br>â€¢ Model cannot beat simple baseline |
| **Best Practices** | â€¢ Establish baseline model performance first<br>â€¢ Test multiple algorithms<br>â€¢ Implement rigorous cross-validation<br>â€¢ Track model performance metrics continuously |

---

### R003: Integration Complexity

| Field | Details |
|-------|---------|
| **Risk ID** | R003 |
| **Category** | Technical - Integration |
| **Description** | Complexity of integrating AI solution with existing enterprise systems exceeds initial estimates |
| **Typical Causes** | Legacy systems, undocumented APIs, security constraints, data format incompatibilities, organizational silos |
| **Typical Impact** | Project delays, increased costs, reduced functionality, technical debt accumulation |
| **Typical Probability** | 3 - Medium (40-50% of enterprise projects) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 9 (Medium Priority) |
| **Mitigation Strategy** | â€¢ Conduct detailed integration analysis during discovery<br>â€¢ Engage all system owners early in planning<br>â€¢ Use enterprise service bus or API gateway<br>â€¢ Build integration spike solutions during prototype<br>â€¢ Allocate 20-30% schedule buffer for integration<br>â€¢ Document all integration points with sequence diagrams<br>â€¢ Use Azure Logic Apps or Data Factory for complex integrations |
| **Contingency Plan** | â€¢ Simplify integration scope (defer non-critical)<br>â€¢ Use batch processing instead of real-time<br>â€¢ Build temporary manual processes as bridge<br>â€¢ Implement facade pattern to isolate complexity<br>â€¢ Engage integration specialists/contractors |
| **Trigger Indicators** | â€¢ API documentation missing or outdated<br>â€¢ Test integrations failing repeatedly<br>â€¢ System owners unresponsive<br>â€¢ Authentication/authorization issues<br>â€¢ Data transformation taking longer than expected |
| **Best Practices** | â€¢ Create integration inventory early<br>â€¢ Use contract-first API design<br>â€¢ Implement comprehensive integration testing<br>â€¢ Build monitoring for all integration points |

---

### R004: Scalability & Performance Issues

| Field | Details |
|-------|---------|
| **Risk ID** | R004 |
| **Category** | Technical - Infrastructure |
| **Description** | Solution cannot handle expected production load or meet response time requirements |
| **Typical Causes** | Underestimated load volumes, inefficient code, resource constraints, architectural limitations, database bottlenecks |
| **Typical Impact** | Poor user experience, system downtime, additional unplanned infrastructure costs, user abandonment |
| **Typical Probability** | 3 - Medium (30-40% of projects) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Define clear performance requirements (response time, throughput, concurrency)<br>â€¢ Conduct load testing early and continuously<br>â€¢ Design for horizontal scalability from start<br>â€¢ Use Azure autoscaling and load balancers<br>â€¢ Implement caching strategies (Redis, CDN)<br>â€¢ Optimize database queries and indexing<br>â€¢ Use asynchronous processing for heavy workloads<br>â€¢ Plan for 3x expected peak load |
| **Contingency Plan** | â€¢ Scale up infrastructure (vertical scaling)<br>â€¢ Implement request throttling and queueing<br>â€¢ Reduce feature scope temporarily<br>â€¢ Implement graceful degradation<br>â€¢ Add read replicas for database |
| **Trigger Indicators** | â€¢ Response time > 3 seconds in tests<br>â€¢ CPU/Memory utilization > 80%<br>â€¢ Failed load tests<br>â€¢ Timeout errors increasing<br>â€¢ Database query time > 1 second |
| **Best Practices** | â€¢ Performance test with 2-3x expected load<br>â€¢ Monitor performance continuously<br>â€¢ Use application performance monitoring (APM) tools<br>â€¢ Implement circuit breakers for resilience |

---

### R005: Security & Compliance Violation

| Field | Details |
|-------|---------|
| **Risk ID** | R005 |
| **Category** | Technical - Security |
| **Description** | Solution fails to meet security standards or regulatory compliance requirements (GDPR, HIPAA, SOC 2, etc.) |
| **Typical Causes** | Incomplete requirements, lack of security expertise, complex regulations, late security reviews, vendor dependencies |
| **Typical Impact** | Legal liability, regulatory fines, project shutdown, reputational damage, data breaches |
| **Typical Probability** | 2 - Low (10-20% if properly managed) |
| **Typical Impact** | 5 - Critical |
| **Typical Risk Score** | 10 (High Priority) |
| **Mitigation Strategy** | â€¢ Engage security and compliance teams from day 1<br>â€¢ Conduct Data Privacy Impact Assessment (DPIA)<br>â€¢ Implement security-by-design principles<br>â€¢ Regular security reviews and penetration testing<br>â€¢ Use Azure Security Center and Defender<br>â€¢ Follow OWASP Top 10 guidelines<br>â€¢ Implement encryption at rest and in transit<br>â€¢ Use Azure Key Vault for secrets management<br>â€¢ Document all compliance requirements |
| **Contingency Plan** | â€¢ Rapid security remediation sprint<br>â€¢ Delay go-live until fully compliant<br>â€¢ Engage external security auditors<br>â€¢ Implement additional controls/compensating controls<br>â€¢ Purchase cybersecurity insurance |
| **Trigger Indicators** | â€¢ Security scan failures<br>â€¢ Compliance gaps identified in audit<br>â€¢ PII exposure detected<br>â€¢ Failed penetration tests<br>â€¢ Regulatory inquiry received |
| **Best Practices** | â€¢ Build security into CI/CD pipeline<br>â€¢ Use infrastructure-as-code with security baselines<br>â€¢ Implement zero-trust architecture<br>â€¢ Regular compliance audits |

---

## CATEGORY 2: DATA RISKS (3 risks)

### R006: Insufficient Training Data

| Field | Details |
|-------|---------|
| **Risk ID** | R006 |
| **Category** | Data - Volume |
| **Description** | Insufficient volume or variety of labeled data to train effective ML models that generalize well |
| **Typical Causes** | Limited historical data, manual labeling required, rare edge cases, imbalanced classes, new use case |
| **Typical Impact** | Model overfitting, poor generalization to production, inability to achieve performance targets |
| **Typical Probability** | 3 - Medium (40-50% of AI projects) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Assess data availability during discovery (need 1000+ samples minimum)<br>â€¢ Use transfer learning from pre-trained models<br>â€¢ Implement data augmentation techniques<br>â€¢ Consider active learning approach<br>â€¢ Budget $50K-$200K for data labeling services<br>â€¢ Explore synthetic data generation<br>â€¢ Partner with data providers if needed<br>â€¢ Use semi-supervised learning techniques |
| **Contingency Plan** | â€¢ Use semi-supervised or self-supervised learning<br>â€¢ Generate synthetic data using GANs<br>â€¢ Reduce model scope to data-rich subset<br>â€¢ Implement few-shot learning techniques<br>â€¢ Collect more data (extend timeline) |
| **Trigger Indicators** | â€¢ Training set < 1000 samples per class<br>â€¢ Class imbalance > 90/10 ratio<br>â€¢ Validation curve not converging<br>â€¢ High variance between train and test performance<br>â€¢ Expert assessment deems data insufficient |
| **Best Practices** | â€¢ Calculate required sample size early (statistical power analysis)<br>â€¢ Track data collection progress<br>â€¢ Prioritize high-quality labels over quantity<br>â€¢ Use cross-validation to maximize data usage |

---

### R007: Data Access & Privacy Constraints

| Field | Details |
|-------|---------|
| **Risk ID** | R007 |
| **Category** | Data - Privacy & Access |
| **Description** | Privacy regulations or data access restrictions prevent use of required data for training or inference |
| **Typical Causes** | GDPR/CCPA requirements, data residency rules, internal data policies, lack of consent, cross-border restrictions |
| **Typical Impact** | Project delays, reduced model scope, need for alternative data sources, compliance violations |
| **Typical Probability** | 3 - Medium (40% of projects) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 9 (Medium Priority) |
| **Mitigation Strategy** | â€¢ Engage legal/compliance team in discovery phase<br>â€¢ Conduct Data Privacy Impact Assessment (DPIA)<br>â€¢ Implement data anonymization/pseudonymization<br>â€¢ Use differential privacy techniques<br>â€¢ Obtain proper data access approvals with documented justification<br>â€¢ Implement data minimization principles<br>â€¢ Use federated learning if data cannot be centralized<br>â€¢ Ensure audit trail for all data access |
| **Contingency Plan** | â€¢ Use aggregated or anonymized data only<br>â€¢ Implement federated learning architecture<br>â€¢ Redesign approach to avoid sensitive data<br>â€¢ Use synthetic data that preserves statistical properties<br>â€¢ Obtain explicit user consent programs |
| **Trigger Indicators** | â€¢ Data access request denied by data owners<br>â€¢ Privacy officer raises compliance concerns<br>â€¢ Cannot obtain required user consent<br>â€¢ Cross-border data transfer blocked<br>â€¢ Legal review identifies GDPR/CCPA issues |
| **Best Practices** | â€¢ Complete DPIA before data access<br>â€¢ Document data usage justification<br>â€¢ Implement data retention policies<br>â€¢ Use privacy-preserving ML techniques |

---

### R008: Data Drift & Model Degradation

| Field | Details |
|-------|---------|
| **Risk ID** | R008 |
| **Category** | Data - Ongoing Operations |
| **Description** | Production data distribution changes over time (concept drift), causing model performance to degrade post-deployment |
| **Typical Causes** | Business environment changes, seasonal patterns, market shifts, user behavior evolution, external events |
| **Typical Impact** | Declining accuracy over time, business value erosion, user trust loss, incorrect predictions |
| **Typical Probability** | 4 - High (60-70% of deployed models) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Implement automated data drift monitoring (statistical tests)<br>â€¢ Set up ML pipeline for automated model retraining<br>â€¢ Define model performance thresholds and alerts<br>â€¢ Create alerting system for drift detection<br>â€¢ Plan regular model refresh cycles (quarterly minimum)<br>â€¢ Use Azure Machine Learning monitoring<br>â€¢ Implement A/B testing for model versions<br>â€¢ Log all predictions for retrospective analysis |
| **Contingency Plan** | â€¢ Trigger emergency model retraining<br>â€¢ Rollback to previous stable model version<br>â€¢ Implement temporary manual review process<br>â€¢ Increase prediction confidence thresholds<br>â€¢ Route uncertain cases to human experts |
| **Trigger Indicators** | â€¢ Model accuracy drops > 5% from baseline<br>â€¢ Prediction distribution shifts significantly<br>â€¢ Business KPIs decline unexpectedly<br>â€¢ Feature distributions change (detected by monitoring)<br>â€¢ User complaints about prediction quality |
| **Best Practices** | â€¢ Monitor model performance daily in production<br>â€¢ Compare current vs. training data distributions<br>â€¢ Set up automated retraining pipeline<br>â€¢ Version all models with rollback capability |

---

## CATEGORY 3: BUSINESS RISKS (4 risks)

### R009: Unclear or Changing Requirements

| Field | Details |
|-------|---------|
| **Risk ID** | R009 |
| **Category** | Business - Requirements |
| **Description** | Business requirements are unclear, incomplete, conflicting, or frequently changing throughout the project |
| **Typical Causes** | Stakeholder misalignment, unclear vision, evolving business understanding, competing priorities, politics |
| **Typical Impact** | Extensive rework, scope creep, budget overruns, timeline delays, stakeholder dissatisfaction, team frustration |
| **Typical Probability** | 4 - High (50-60% of projects) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Invest 3-4 weeks in thorough discovery with workshops<br>â€¢ Use interactive prototypes to clarify requirements early<br>â€¢ Implement formal change control process<br>â€¢ Conduct bi-weekly stakeholder reviews<br>â€¢ Document all requirements in BRD with approval<br>â€¢ Use user stories and acceptance criteria<br>â€¢ Apply Five Whys to understand root problems<br>â€¢ Create requirements traceability matrix |
| **Contingency Plan** | â€¢ Freeze scope after discovery phase<br>â€¢ Push all changes to Phase 2 backlog<br>â€¢ Escalate conflicts to executive sponsor<br>â€¢ Conduct emergency requirements workshop<br>â€¢ Implement agile approach with shorter sprints |
| **Trigger Indicators** | â€¢ Multiple change requests per week<br>â€¢ Stakeholders disagreeing on priorities<br>â€¢ Prototype feedback contradictory<br>â€¢ Requirements document has >10 revisions<br>â€¢ Team confused about objectives |
| **Best Practices** | â€¢ Get written approval for requirements<br>â€¢ Use prototypes to validate understanding<br>â€¢ Document assumptions explicitly<br>â€¢ Manage scope with change control board |

---

### R010: Lack of Executive Sponsorship

| Field | Details |
|-------|---------|
| **Risk ID** | R010 |
| **Category** | Business - Governance |
| **Description** | Insufficient or declining executive support and engagement throughout project lifecycle |
| **Typical Causes** | Competing priorities, executive turnover, lack of AI understanding, political resistance, budget pressures |
| **Typical Impact** | Blocked decisions, resource constraints, budget cuts, project cancellation, lack of organizational support |
| **Typical Probability** | 2 - Low (15-20% if managed well) |
| **Typical Impact** | 5 - Critical |
| **Typical Risk Score** | 10 (High Priority) |
| **Mitigation Strategy** | â€¢ Secure strong, committed sponsor before project start<br>â€¢ Monthly executive steering committee meetings<br>â€¢ Weekly executive dashboard/status updates<br>â€¢ Demonstrate quick wins and tangible value early<br>â€¢ Keep sponsor informed of all major risks<br>â€¢ Build steering committee with multiple executives<br>â€¢ Use executive coaching guide for engagement<br>â€¢ Align project to executive's strategic objectives |
| **Contingency Plan** | â€¢ Escalate to higher-level executive (CEO/Board)<br>â€¢ Seek alternative sponsor in organization<br>â€¢ Re-present business case with updated ROI<br>â€¢ Demonstrate competitor risk if not proceeding<br>â€¢ Pause project until sponsorship secured |
| **Trigger Indicators** | â€¢ Sponsor missing >2 consecutive meetings<br>â€¢ Decisions delayed > 2 weeks consistently<br>â€¢ Budget approvals stalled<br>â€¢ Sponsor not responding to communications<br>â€¢ Sponsor announces departure/role change |
| **Best Practices** | â€¢ Get written sponsor commitment upfront<br>â€¢ Keep sponsor engaged with monthly touchpoints<br>â€¢ Celebrate wins publicly<br>â€¢ Use sponsor to remove organizational blockers |

---

### R011: User Adoption Resistance

| Field | Details |
|-------|---------|
| **Risk ID** | R011 |
| **Category** | Business - Change Management |
| **Description** | End users resist adopting the AI solution due to fear, skepticism, workflow disruption, or poor user experience |
| **Typical Causes** | Fear of job loss, lack of trust in AI, poor UX, inadequate training, "not invented here" syndrome, change fatigue |
| **Typical Impact** | Low usage rates, business benefits not realized, project deemed failure, negative ROI, user workarounds |
| **Typical Probability** | 4 - High (60% of AI projects face some resistance) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 16 (Critical Priority) |
| **Mitigation Strategy** | â€¢ Invest in comprehensive change management from project start<br>â€¢ Involve users early in design (co-creation workshops)<br>â€¢ Build comprehensive training program (hands-on, role-based)<br>â€¢ Emphasize augmentation not replacement messaging<br>â€¢ Ensure excellent, intuitive user experience<br>â€¢ Create champions/advocate network (10% of users)<br>â€¢ Provide ongoing support (help desk, office hours)<br>â€¢ Celebrate user success stories<br>â€¢ Make adoption gradual with opt-in period |
| **Contingency Plan** | â€¢ Intensive one-on-one coaching and support<br>â€¢ Adjust workflow integration based on feedback<br>â€¢ Make adoption gradual or optional initially<br>â€¢ Implement incentive programs for early adopters<br>â€¢ Address job security concerns directly with leadership |
| **Trigger Indicators** | â€¢ Negative feedback in UAT sessions<br>â€¢ Low training attendance (<70%)<br>â€¢ Rumors or concerns raised in town halls<br>â€¢ Usage metrics below 50% in pilot<br>â€¢ Complaints to management about "being forced" |
| **Best Practices** | â€¢ Start change management in mobilization phase<br>â€¢ Conduct change impact assessment<br>â€¢ Track adoption metrics daily post-launch<br>â€¢ Provide multiple training modalities |

---

### R012: Unrealistic ROI Expectations

| Field | Details |
|-------|---------|
| **Risk ID** | R012 |
| **Category** | Business - Value Realization |
| **Description** | Stakeholders have unrealistic expectations about AI capabilities, timeline to value, or ROI magnitude |
| **Typical Causes** | AI hype in media, misunderstanding of limitations, overly optimistic business case, lack of AI literacy |
| **Typical Impact** | Project perceived as failure even if technically successful, budget cuts, loss of trust in AI initiatives |
| **Typical Probability** | 3 - Medium (30-40% of projects) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 9 (Medium Priority) |
| **Mitigation Strategy** | â€¢ Set realistic expectations early with education sessions<br>â€¢ Educate stakeholders on AI capabilities AND limitations<br>â€¢ Build conservative business case (70-80% of theoretical max)<br>â€¢ Define clear, measurable success criteria<br>â€¢ Demonstrate value incrementally with quick wins<br>â€¢ Show industry benchmarks and case studies<br>â€¢ Use AI Readiness Assessment to calibrate expectations<br>â€¢ Document assumptions transparently |
| **Contingency Plan** | â€¢ Re-baseline expectations with revised business case<br>â€¢ Showcase achieved benefits even if below original target<br>â€¢ Extend benefit realization timeline<br>â€¢ Pivot messaging to qualitative benefits<br>â€¢ Conduct AI education workshops |
| **Trigger Indicators** | â€¢ Stakeholder comments suggesting 100% automation<br>â€¢ Business case shows >500% ROI in Year 1<br>â€¢ Expectations of immediate accuracy improvement<br>â€¢ Lack of understanding that AI requires training data<br>â€¢ Comparing to consumer AI (ChatGPT) expectations |
| **Best Practices** | â€¢ Use ROI calculator with conservative assumptions<br>â€¢ Show ramp-up period (60% in Y1, 90% in Y2)<br>â€¢ Set success criteria collaboratively<br>â€¢ Track and report benefits monthly |

---

## CATEGORY 4: ORGANIZATIONAL RISKS (3 risks)

### R013: Resource Availability

| Field | Details |
|-------|---------|
| **Risk ID** | R013 |
| **Category** | Organizational - Resources |
| **Description** | Key resources (AI experts, data scientists, SMEs, business owners) not available when needed |
| **Typical Causes** | Competing projects, resource turnover, limited AI talent pool, budget constraints, poor resource planning |
| **Typical Impact** | Project delays, quality issues, increased costs from contractors, knowledge gaps, team burnout |
| **Typical Probability** | 3 - Medium (40% of projects) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Secure written resource commitments before project start<br>â€¢ Cross-train team members on critical skills<br>â€¢ Budget for contractors/consultants (15-20% of budget)<br>â€¢ Build 20% resource buffer into schedule<br>â€¢ Create resource escalation process to leadership<br>â€¢ Use RACI matrix to clarify commitments<br>â€¢ Identify backup resources for key roles<br>â€¢ Conduct capacity planning with resource managers |
| **Contingency Plan** | â€¢ Engage external contractors immediately<br>â€¢ Adjust timeline to match resource availability<br>â€¢ Reduce scope to match available resources<br>â€¢ Escalate to executive sponsor for resource prioritization<br>â€¢ Reallocate resources from lower priority work |
| **Trigger Indicators** | â€¢ Resources allocated < 50% of promised time<br>â€¢ Key resource announces resignation<br>â€¢ Competing project pulling resources<br>â€¢ Team members working >50 hours/week<br>â€¢ Resource manager not responding to requests |
| **Best Practices** | â€¢ Get resource commitments in writing (RACI)<br>â€¢ Track actual vs. planned resource allocation<br>â€¢ Identify resource risks early<br>â€¢ Build bench of pre-approved contractors |

---

### R014: Lack of AI/ML Expertise

| Field | Details |
|-------|---------|
| **Risk ID** | R014 |
| **Category** | Organizational - Skills |
| **Description** | Team lacks necessary AI/ML expertise, best practices knowledge, or domain experience to deliver successfully |
| **Typical Causes** | Limited internal AI capability, emerging technology, skill gaps in team, underestimating complexity |
| **Typical Impact** | Poor technical decisions, suboptimal solutions, extended timeline, quality issues, technical debt |
| **Typical Probability** | 3 - Medium (40-50% of first AI projects) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Assess skill gaps during project planning (skills matrix)<br>â€¢ Provide comprehensive training and upskilling ($10K-$30K)<br>â€¢ Engage external AI consultants for critical phases<br>â€¢ Partner with Microsoft AI specialists<br>â€¢ Use proven ML frameworks and Azure managed services<br>â€¢ Implement peer code reviews and architecture reviews<br>â€¢ Bring in industry experts for workshops<br>â€¢ Consider AI Center of Excellence model |
| **Contingency Plan** | â€¢ Increase external consultant engagement (add budget)<br>â€¢ Simplify technical approach to match team skills<br>â€¢ Use more Azure managed services (less custom code)<br>â€¢ Hire experienced AI/ML engineer<br>â€¢ Partner with SI firm for delivery |
| **Trigger Indicators** | â€¢ Team struggling with basic ML concepts<br>â€¢ Multiple technical pivots without clear rationale<br>â€¢ External expert recommendations consistently ignored<br>â€¢ Code quality issues in reviews<br>â€¢ Unable to explain model decisions |
| **Best Practices** | â€¢ Complete skills assessment before project<br>â€¢ Create training plan for identified gaps<br>â€¢ Pair junior team members with experts<br>â€¢ Budget for external expertise from start |

---

### R015: Vendor/Third-Party Dependency

| Field | Details |
|-------|---------|
| **Risk ID** | R015 |
| **Category** | Organizational - External Dependencies |
| **Description** | Critical dependency on external vendors or partners (Microsoft, data providers, SI partners) causes delays or issues |
| **Typical Causes** | Vendor delays, service outages, contract issues, vendor prioritization, lack of vendor responsiveness |
| **Typical Impact** | Project delays, blocked workstreams, potential cost increases, reduced functionality |
| **Typical Probability** | 2 - Low (15-20% with good vendor management) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 6 (Low Priority) |
| **Mitigation Strategy** | â€¢ Identify all vendor dependencies in discovery phase<br>â€¢ Build relationships with vendor account teams early<br>â€¢ Include vendors in project planning and timelines<br>â€¢ Ensure contractual SLAs for critical dependencies<br>â€¢ Build 2-week schedule buffers around vendor dependencies<br>â€¢ Have backup vendors identified where possible<br>â€¢ Regular check-ins with vendor teams<br>â€¢ Escalation paths to vendor management documented |
| **Contingency Plan** | â€¢ Escalate to vendor account manager/partner manager<br>â€¢ Use alternative vendors if available<br>â€¢ Adjust project schedule around vendor delays<br>â€¢ Build temporary workaround solutions<br>â€¢ Negotiate penalty clauses for missed commitments |
| **Trigger Indicators** | â€¢ Vendor missing agreed commitments or deadlines<br>â€¢ Azure service issues impacting development<br>â€¢ Contract negotiations stalled<br>â€¢ Vendor unresponsive to inquiries<br>â€¢ Vendor announces product discontinuation |
| **Best Practices** | â€¢ Map all vendor dependencies in project plan<br>â€¢ Establish vendor contacts before project start<br>â€¢ Review SLAs and support tier<br>â€¢ Monitor vendor health/stability |

---

## CATEGORY 5: PROJECT MANAGEMENT RISKS (3 risks)

### R016: Scope Creep

| Field | Details |
|-------|---------|
| **Risk ID** | R016 |
| **Category** | Project Management - Scope Control |
| **Description** | Uncontrolled expansion of project scope without corresponding adjustments to time, cost, and resources |
| **Typical Causes** | Weak change control, stakeholder pressure, unclear initial scope, "while we're at it" syndrome, gold plating |
| **Typical Impact** | Budget overruns, schedule delays, team burnout, quality degradation, missed deadlines |
| **Typical Probability** | 4 - High (60% of projects) |
| **Typical Impact** | 3 - Medium |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Define clear scope baseline with in/out of scope lists<br>â€¢ Implement formal change control board (CCB)<br>â€¢ Bi-weekly scope reviews with stakeholders<br>â€¢ Manage stakeholder expectations proactively<br>â€¢ Document all changes with impact analysis (time, cost, risk)<br>â€¢ Require executive approval for scope changes<br>â€¢ Create Phase 2 backlog for new requests<br>â€¢ Use agile methodology with time-boxed sprints |
| **Contingency Plan** | â€¢ Freeze scope temporarily (no new changes)<br>â€¢ Defer all new requests to Phase 2<br>â€¢ Escalate to executive sponsor for prioritization<br>â€¢ Re-baseline project plan with new scope<br>â€¢ Negotiate timeline or resource increase |
| **Trigger Indicators** | â€¢ Frequent "small" additions that accumulate<br>â€¢ Schedule slipping consistently<br>â€¢ Team working overtime regularly<br>â€¢ Change requests > 5 per sprint<br>â€¢ Scope baseline document outdated |
| **Best Practices** | â€¢ Get formal scope approval in charter<br>â€¢ Use change request template for all changes<br>â€¢ Track cumulative impact of changes<br>â€¢ Say "yes, in Phase 2" to new requests |

---

### R017: Inadequate Testing

| Field | Details |
|-------|---------|
| **Risk ID** | R017 |
| **Category** | Project Management - Quality Assurance |
| **Description** | Insufficient testing time, coverage, or rigor leads to production defects and quality issues |
| **Typical Causes** | Schedule pressure, underestimating testing effort, lack of test resources, compressing QA to meet dates |
| **Typical Impact** | Production defects, poor user experience, emergency fixes, reputation damage, user trust loss |
| **Typical Probability** | 3 - Medium (30-40% of projects) |
| **Typical Impact** | 4 - High |
| **Typical Risk Score** | 12 (High Priority) |
| **Mitigation Strategy** | â€¢ Allocate adequate testing time (20-25% of build phase)<br>â€¢ Define comprehensive test strategy covering all levels<br>â€¢ Automate regression testing (save 50% effort)<br>â€¢ Involve users in UAT early and extensively<br>â€¢ Never compress testing to meet arbitrary dates<br>â€¢ Implement continuous testing in CI/CD pipeline<br>â€¢ Test with production-like data volumes<br>â€¢ Include bias and fairness testing for AI models |
| **Contingency Plan** | â€¢ Delay go-live to complete adequate testing<br>â€¢ Implement phased rollout to limit blast radius<br>â€¢ Enhanced hypercare support (24/7 for 2 weeks)<br>â€¢ Increase test resources (add contractors)<br>â€¢ Focus on critical path testing only |
| **Trigger Indicators** | â€¢ Test coverage < 70% of requirements<br>â€¢ UAT feedback rate declining<br>â€¢ Pressure from leadership to skip test phases<br>â€¢ Defects found in production not caught in testing<br>â€¢ Test team working excessive overtime |
| **Best Practices** | â€¢ Create comprehensive test strategy early<br>â€¢ Use test-driven development (TDD)<br>â€¢ Automate everything that can be automated<br>â€¢ Track test coverage and defect metrics |

---

### R018: Communication Breakdown

| Field | Details |
|-------|---------|
| **Risk ID** | R018 |
| **Category** | Project Management - Communication |
| **Description** | Poor communication leading to misunderstandings, misalignment, duplicated effort, and errors |
| **Typical Causes** | Distributed teams, unclear communication channels, information overload, cultural differences, silos |
| **Typical Impact** | Rework, confusion, delays, stakeholder dissatisfaction, team conflict, missed requirements |
| **Typical Probability** | 3 - Medium (30-40% of distributed projects) |
| **Typical Impact** | 2 - Low |
| **Typical Risk Score** | 6 (Low Priority) |
| **Mitigation Strategy** | â€¢ Establish clear communication plan with RACI<br>â€¢ Daily standups for core team (15 min max)<br>â€¢ Weekly status meetings with extended team<br>â€¢ Use Microsoft Teams/SharePoint for collaboration<br>â€¢ Document all key decisions in decision log<br>â€¢ Create feedback loops and check-in points<br>â€¢ Use visual management (dashboards, Kanban boards)<br>â€¢ Establish "single source of truth" for project docs |
| **Contingency Plan** | â€¢ Increase meeting frequency temporarily<br>â€¢ Conduct one-on-one stakeholder meetings<br>â€¢ Communication audit and reset<br>â€¢ Bring distributed team together for in-person workshop<br>â€¢ Clarify communication protocols |
| **Trigger Indicators** | â€¢ Team members working on conflicting solutions<br>â€¢ Stakeholders surprised by project status<br>â€¢ Requirements misunderstood<br>â€¢ Decisions being questioned or re-litigated<br>â€¢ Email threads >10 people with confusion |
| **Best Practices** | â€¢ Create communication plan in mobilization<br>â€¢ Use standard meeting agendas<br>â€¢ Send meeting notes within 24 hours<br>â€¢ Over-communicate in distributed teams |

---

## Quick Reference Matrix

### Risk Priority Summary

| Risk ID | Risk Name | Category | Typical Score | Frequency |
|---------|-----------|----------|---------------|-----------|
| **R011** | User Adoption Resistance | Business | 16 (Critical) | 60% |
| **R001** | Inadequate Data Quality | Technical | 16 (High) | 70% |
| **R002** | Model Performance Issues | Technical | 15 (High) | 40% |
| **R016** | Scope Creep | PM | 12 (High) | 60% |
| **R004** | Scalability Issues | Technical | 12 (High) | 35% |
| **R006** | Insufficient Training Data | Data | 12 (High) | 45% |
| **R008** | Data Drift | Data | 12 (High) | 65% |
| **R009** | Unclear Requirements | Business | 12 (High) | 55% |
| **R013** | Resource Availability | Organizational | 12 (High) | 40% |
| **R014** | Lack of AI Expertise | Organizational | 12 (High) | 50% |
| **R017** | Inadequate Testing | PM | 12 (High) | 35% |
| **R005** | Security/Compliance | Technical | 10 (High) | 15% |
| **R010** | Lack of Sponsorship | Business | 10 (High) | 20% |
| **R003** | Integration Complexity | Technical | 9 (Medium) | 45% |
| **R007** | Data Access Constraints | Data | 9 (Medium) | 40% |
| **R012** | Unrealistic ROI | Business | 9 (Medium) | 35% |
| **R015** | Vendor Dependency | Organizational | 6 (Low) | 20% |
| **R018** | Communication Breakdown | PM | 6 (Low) | 35% |

---

## Risk Categories Summary

| Category | # of Risks | Avg Priority | Top Risk |
|----------|-----------|--------------|----------|
| **Technical** | 5 | High | R001 - Data Quality (16) |
| **Data** | 3 | High | R006 - Insufficient Data (12) |
| **Business** | 4 | High | R011 - User Adoption (16) |
| **Organizational** | 3 | High | R013/R014 - Resources/Skills (12) |
| **Project Management** | 3 | High | R016 - Scope Creep (12) |

---

## How to Customize for Your Project

### Step 1: Select Relevant Risks
Review all 18 risks and select those applicable to your project context. Not all risks apply to all projects.

### Step 2: Adjust Probability & Impact
Update probability and impact based on:
- Your organization's maturity
- Project complexity
- Team experience
- Risk appetite

### Step 3: Customize Mitigation
Tailor mitigation strategies to:
- Your available resources
- Your organizational processes
- Your risk tolerance
- Your timeline constraints

### Step 4: Add Project-Specific Risks
Add 3-5 risks unique to your project:
- Industry-specific regulations
- Legacy system constraints
- Organizational politics
- Market timing

### Step 5: Assign Owners
Assign a risk owner for each risk who:
- Monitors trigger indicators
- Executes mitigation strategies
- Reports status weekly
- Escalates when needed

---

## Related Resources

### Internal
- **[Risk Register Template](../05-risk-register.md)** - Project-specific risk tracking
- **[Business Case Template](../02-business-case.md)** - Includes risk section
- **[ROI Analysis](./roi-financial-analysis.md)** - Financial risk assessment

### External
- [Project Management Institute - Risk Management](https://www.pmi.org/learning/library/risk-analysis-project-management-7070)
- [AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)

---

## Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2026-01-13 | PM | Initial risk library created with 18 pre-defined risks |

---

**Questions or Improvements?**  
Open an issue or submit a PR to add more risks or improve mitigation strategies!

**Last Updated**: January 13, 2026
