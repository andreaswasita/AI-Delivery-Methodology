# How to Use the AI Maturity Assessment
## A Prescriptive Guide for the AI Delivery Methodology

---

## ðŸ“‹ Table of Contents
1. [Overview](#1-overview)
2. [When to Use This Assessment](#2-when-to-use-this-assessment)
3. [Who Should Complete It](#3-who-should-complete-it)
4. [Step-by-Step Instructions](#4-step-by-step-instructions)
5. [Interpreting Your Results](#5-interpreting-your-results)
6. [Methodology Integration](#6-methodology-integration)
7. [Action Planning Based on Maturity Level](#7-action-planning-based-on-maturity-level)
8. [Using Results with Other Tools](#8-using-results-with-other-tools)
9. [Best Practices](#9-best-practices)
10. [Common Pitfalls to Avoid](#10-common-pitfalls-to-avoid)

---

## 1. Overview

### 1.1 What is the AI Maturity Assessment?

The **AI Maturity Assessment** is a research-backed diagnostic tool that evaluates your organization's readiness and capability to successfully deliver AI initiatives. Based on frameworks from Accenture, Gartner, McKinsey, PwC, and other leading institutions, it measures maturity across five critical dimensions:

- **Strategy & Leadership**: Vision, governance, and organizational commitment
- **Data & Infrastructure**: Data readiness and technical foundation
- **Technology & Operations**: AI/ML capabilities and operational maturity
- **Talent & Culture**: Skills, collaboration, and innovation mindset
- **Governance & Ethics**: Responsible AI practices and risk management

### 1.2 Why This Matters

**Industry Reality:**
- **Only 12% of organizations** reach Level 4-5 maturity (Accenture, 2024)
- **Mature organizations are 3x more likely** to achieve AI ROI targets
- **70% of AI projects fail** due to organizational readiness issues, not technology
- **Maturity Level predicts success rate**: Level 1-2 (15% success), Level 3 (45% success), Level 4-5 (80% success)

**Your Maturity Level Determines:**
- âœ“ Which AI use cases you should pursue first
- âœ“ What team structure and skills you need
- âœ“ How aggressive your timeline can be
- âœ“ Where to invest in capability building
- âœ“ What risks require immediate mitigation
- âœ“ Your realistic ROI expectations

---

## 2. When to Use This Assessment

### 2.1 Required Usage Points (Mandatory)

#### ðŸŽ¯ **Before Business Envisioning Workshop (Pre-Sales/Discovery)**
**Timing**: 1-2 weeks before the workshop  
**Purpose**: Establish baseline organizational readiness  
**Who Completes**: Executive Sponsor, IT/Data Leaders, Business Unit Leaders  
**Output Used For**:
- Tailoring workshop content to maturity level
- Setting realistic expectations for timeline and scope
- Identifying capability gaps that need addressing
- Determining appropriate engagement model (see Section 6.2)

**Action**: Use the **[AI Maturity Assessment Tool](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/ai-maturity-assessment.html)**

---

#### ðŸ” **During Mobilisation Phase (Week 1)**
**Timing**: First week of project kickoff  
**Purpose**: Validate assessment with full project team perspective  
**Who Completes**: Expanded team including technical architects, data engineers, ML engineers  
**Output Used For**:
- Confirming or adjusting project approach based on technical view
- Identifying specific technical capability gaps
- Planning training and upskilling activities
- Setting up governance structures appropriate to maturity

**Action**: Reassess with technical team; compare results with initial assessment

---

#### ðŸ“Š **During Discovery Phase (Week 2-3)**
**Timing**: As part of readiness assessment activities  
**Purpose**: Deep-dive into specific dimension weaknesses  
**Who Completes**: Capability area owners for low-scoring dimensions  
**Output Used For**:
- Creating detailed capability build plans
- Prioritizing infrastructure investments
- Identifying quick wins to boost maturity
- Setting dimension-specific improvement milestones

**Action**: Conduct dimension-specific workshops for scores below 3.0

---

### 2.2 Recommended Usage Points (Best Practice)

#### ðŸ—ï¸ **Before Major Phase Transitions**
**Phases**: Before Build, Deploy, Operate phases  
**Purpose**: Verify organization has evolved to handle next phase complexity  
**Output Used For**:
- Gate decision: proceed, delay, or adjust scope
- Identifying new risks that emerged
- Adjusting team structure or engagement model
- Demonstrating improvement trajectory to sponsors

---

#### ðŸ“ˆ **Quarterly Reviews (Ongoing Projects)**
**Timing**: Every 3 months during project execution  
**Purpose**: Track maturity improvement over time  
**Output Used For**:
- Measuring capability development progress
- Celebrating wins and identifying stagnation
- Adjusting support and governance models
- Justifying continued investment in capability building

---

#### ðŸŽ“ **Post-Project Review**
**Timing**: 30-60 days after go-live  
**Purpose**: Measure how much organization maturity improved  
**Output Used For**:
- Documenting lessons learned
- Quantifying capability lift for next project
- Justifying AI Center of Excellence investment
- Planning next AI initiatives with higher confidence

---

## 3. Who Should Complete It

### 3.1 Primary Assessment Team (Initial Baseline)

**Required Participants:**
1. **Executive Sponsor** (CEO/CIO/CDO/Business Unit Lead)
   - Strategy & Leadership questions
   - Governance & Ethics questions (shared)

2. **Chief Data Officer / Data Leader**
   - Data & Infrastructure questions
   - Technology & Operations questions (shared)

3. **Head of IT/Technology**
   - Technology & Operations questions
   - Data & Infrastructure questions (shared)

4. **HR/Talent Leader**
   - Talent & Culture questions

5. **Chief Risk Officer / Compliance Leader**
   - Governance & Ethics questions (shared)

**Assessment Method**: 
- **Option A (Recommended)**: Collaborative session (90 minutes) with all participants discussing each question
- **Option B**: Individual assessments averaged, then discussed to resolve gaps >1 point

---

### 3.2 Technical Deep-Dive Team (Mobilisation Phase)

**Additional Participants:**
- Solution Architect / Technical Lead
- Data Engineering Lead
- ML Engineering Lead (if Level 3+)
- DevOps/MLOps Lead
- Security Architect

**Purpose**: Validate and refine technical dimension scores with hands-on practitioners

---

### 3.3 Assessment Facilitation

**Who Should Facilitate:**
- âœ“ External consultant (most objective)
- âœ“ PMO/Governance team (if independent)
- âœ“ AI Center of Excellence lead
- âœ— Project team member (avoid bias)
- âœ— Executive sponsor directly (avoid aspirational scoring)

---

## 4. Step-by-Step Instructions

### Phase 1: Preparation (30 minutes)

**Step 1: Schedule the Assessment**
- Block 90-120 minutes with key stakeholders
- Send pre-read: This guide + assessment tool link
- Ask participants to review questions beforehand (but not answer yet)

**Step 2: Set Context**
- Explain purpose: honest baseline, not performance review
- Emphasize: "Aspirational scoring helps no one - accuracy drives better outcomes"
- Share: Industry statistics (only 12% at Level 4-5)
- Establish: Confidentiality and psychological safety

**Step 3: Review Maturity Level Definitions**
Open [AI Maturity Assessment Tool](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/ai-maturity-assessment.html) and review:
- Level 1 (Initial/Aware): Ad-hoc, siloed, exploring
- Level 2 (Developing/Experimenting): Structured pilots, emerging capabilities
- Level 3 (Defined/Emerging): Repeatable processes, production at scale
- Level 4 (Managed/Performing): Optimized operations, strategic advantage
- Level 5 (Optimizing/Leading): Industry-leading, continuous innovation

---

### Phase 2: Complete Assessment (60 minutes)

**Step 4: Section-by-Section Completion**
For each of the 5 dimensions (3 questions each):

1. **Read question aloud**
2. **Review all 5 answer options** (Facilitator reads Level 1, 3, 5 descriptions)
3. **Initial individual scoring** (everyone writes their score privately)
4. **Share scores** (go around the room)
5. **Discuss gaps >1 point** until consensus
6. **Select final answer** in the tool
7. **Move to next question**

**Timing Guide:**
- Strategy & Leadership: 15 minutes
- Data & Infrastructure: 15 minutes
- Technology & Operations: 15 minutes
- Talent & Culture: 10 minutes
- Governance & Ethics: 15 minutes
- Buffer/Discussion: 10 minutes

**Pro Tip**: If team can't decide between two adjacent levels (e.g., Level 2 vs 3), always choose the **lower** level. Progress requires evidence, not aspiration.

---

### Phase 3: Results Review (30 minutes)

**Step 5: Calculate & Review Overall Maturity**
- Click "Calculate Maturity" button
- Review overall score and maturity level
- Note: Averages can be deceptive (3.2 could mean consistent 3s or mix of 1s and 5s)

**Step 6: Analyze Dimension Scores**
Identify:
- **Strengths**: Dimensions at or above overall average
- **Critical Gaps**: Dimensions >0.5 points below overall average
- **Outliers**: Any dimension that scored 1.0-1.5 (immediate risk)
- **Bright Spots**: Any dimension that scored 4.0+ (leverage this)

**Step 7: Review Recommendations**
- Read level-specific recommendations
- Prioritize dimension-specific actions
- Flag recommendations requiring executive approval/budget

---

### Phase 4: Action Planning (30-60 minutes)

**Step 8: Document Key Insights**
Use the **Copy Results** button to capture:
- Overall maturity level
- Dimension scores
- Priority improvement areas
- Top 3 recommendations

**Step 9: Map to Methodology Phases**
Based on your maturity level, determine:
- Which methodology phases need adjustment? (See Section 6.1)
- What tools should you use? (See Section 8)
- What engagement model fits? (See Section 6.2)
- What risks to escalate immediately?

**Step 10: Create Improvement Roadmap**
See Section 7 for maturity-level-specific action plans

---

## 5. Interpreting Your Results

### 5.1 Overall Maturity Level

| Level | Score Range | Interpretation | Project Success Rate |
|-------|-------------|----------------|---------------------|
| **Level 1: Initial/Aware** | 1.0 - 1.8 | AI is exploratory; significant capability building needed before production AI | ~15% |
| **Level 2: Developing** | 1.9 - 2.6 | Some structure exists; ready for pilots but not production at scale | ~30% |
| **Level 3: Defined/Emerging** | 2.7 - 3.4 | Repeatable processes; can deliver production AI with guidance | ~45% |
| **Level 4: Managed** | 3.5 - 4.2 | Optimized operations; consistent delivery, strategic AI advantage | ~70% |
| **Level 5: Optimizing/Leading** | 4.3 - 5.0 | Industry-leading AI maturity; innovation engine | ~85% |

### 5.2 Dimension Analysis

#### Strategy & Leadership
**High Score (4.0+)**: Strong executive backing, clear AI strategy, demonstrates organizational commitment
**Low Score (<2.5)**: Major risk factor; projects will struggle without vision and sponsorship
**Critical Actions if Low**: Conduct Executive Coaching (see Guide 90), establish AI governance council

#### Data & Infrastructure  
**High Score (4.0+)**: Data assets are accessible and trustworthy; infrastructure can support AI
**Low Score (<2.5)**: Fundamental blocker; AI projects will be slow and costly without data readiness
**Critical Actions if Low**: Data quality assessment, infrastructure uplift, establish data governance

#### Technology & Operations
**High Score (4.0+)**: Technical capability to build, deploy, and operate AI; MLOps maturity
**Low Score (<2.5)**: Can't sustain production AI; projects will not reach or maintain production
**Critical Actions if Low**: MLOps foundation (see Guide 03), DevOps setup, CI/CD pipeline establishment

#### Talent & Culture
**High Score (4.0+)**: Right skills in place; culture supports innovation and experimentation
**Low Score (<2.5)**: Skill gaps will bottleneck delivery; resistance will undermine adoption
**Critical Actions if Low**: Upskilling program, hire key talent, change management intensive approach

#### Governance & Ethics
**High Score (4.0+)**: Responsible AI practices embedded; risk management mature
**Low Score (<2.5)**: Compliance risk; reputational risk; ethical issues may emerge post-deployment
**Critical Actions if Low**: Establish AI ethics board, responsible AI framework, model risk management (see Guide 96)

---

### 5.3 Red Flags (Immediate Action Required)

**ðŸš¨ Any Dimension Scoring 1.0-1.5**
- **This is a project blocker** for that capability area
- **Action**: Create 30-60-90 day improvement plan before proceeding to Build phase
- **Consider**: Delaying production deployment until this dimension reaches 2.5+

**ðŸš¨ Gap >1.5 Between Highest and Lowest Dimension**
- **Risk**: Weakest dimension will bottleneck the project
- **Action**: Balance investment - don't over-invest in strengths while ignoring weaknesses
- **Consider**: Tailored engagement model with extra support for weak areas

**ðŸš¨ Technology Score >1 Point Above Talent Score**
- **Risk**: "All tools, no skills" - won't be able to leverage the technology
- **Action**: Immediate upskilling and talent acquisition plan
- **Consider**: Staff augmentation or managed services for delivery

**ðŸš¨ Strategy Score <2.0**
- **Risk**: Project will lose funding or support mid-stream
- **Action**: Pause project; conduct Executive Coaching workshop (Guide 90)
- **Consider**: Whether this is the right time to launch an AI initiative

---

## 6. Methodology Integration

### 6.1 Adjusting Methodology Phases Based on Maturity

#### **Level 1 (Initial/Aware) - Modified Approach**

**ðŸŽ¯ Business Envisioning Workshop**
- **Duration**: Extend to 2 full days (vs standard 1 day)
- **Focus**: 60% AI education, 40% use case ideation
- **Deliverable**: Learning agenda + 1-2 simple use cases (not full roadmap)
- **Tools**: Start with Value Analysis Chatbot for education, skip ROI Calculator initially

**ðŸ” Mobilisation Phase**
- **Duration**: Extend to 6-8 weeks (vs standard 2-4 weeks)
- **Add Activity**: AI 101 bootcamp for leadership (1 week)
- **Add Activity**: Data readiness assessment (2 weeks)
- **Critical**: Establish AI governance council before proceeding

**ðŸ—ï¸ Build Phase**
- **Approach**: **Start with Hackathon/POC only** - do NOT proceed to production build
- **Duration**: 2-4 week time-boxed hackathon
- **Success Criteria**: Learning and capability building, NOT production deployment
- **Next Step**: Re-assess maturity after hackathon; build capability before production

**âœ… Recommendation**: 
- Treat first project as a **learning initiative**
- Invest in AI Center of Excellence establishment (Guide 91)
- Target 6-12 months to reach Level 2 before production AI

---

#### **Level 2 (Developing/Experimenting) - Guided Approach**

**ðŸŽ¯ Business Envisioning Workshop**
- **Duration**: Standard 1 day
- **Focus**: 30% capability review, 70% use case definition
- **Deliverable**: Prioritized use case roadmap (3-5 use cases)
- **Tools**: Use all calculators; prioritize based on ROI and feasibility

**ðŸ” Mobilisation Phase**
- **Duration**: Standard 2-4 weeks
- **Add Activity**: Detailed capability gap analysis (use this assessment)
- **Critical**: Establish MLOps foundation (Guide 03) if not exists

**ðŸ—ï¸ Build Phase**
- **Approach**: Pilot project with **managed support** (external partner or SI)
- **Duration**: 3-6 months for first production model
- **Risk Mitigation**: Heavyweight governance; frequent checkpoints
- **Engagement Mode**: Staff augmentation or co-delivery (see Section 6.2)

**ðŸš€ Deploy Phase**
- **Approach**: Controlled rollout with limited user base
- **Duration**: 1-2 months pilot before full production
- **Critical**: Establish model monitoring and incident response

**âœ… Recommendation**: 
- Choose **low-complexity, high-impact** use case for first project
- Plan for 30-40% more time than Level 3 organization would need
- Invest heavily in knowledge transfer to build internal capability

---

#### **Level 3 (Defined/Emerging) - Standard Approach**

**ðŸŽ¯ Business Envisioning Workshop**
- **Duration**: Standard 1 day
- **Focus**: 10% capability review, 90% use case prioritization and roadmapping
- **Deliverable**: Comprehensive AI roadmap (5-10 use cases, 12-18 months)
- **Tools**: Use all calculators; create business case for top 3 use cases

**ðŸ” Mobilisation Phase**
- **Duration**: Standard 2-4 weeks
- **Focus**: Standard activities - no major capability building needed

**ðŸ—ï¸ Build Phase**
- **Approach**: Full agile delivery with internal team + selective external expertise
- **Duration**: 3-4 months for production model
- **Engagement Mode**: Advisory + selective staff augmentation (see Section 6.2)

**ðŸš€ Deploy Phase**
- **Approach**: Phased rollout; can proceed to full production
- **Duration**: 4-8 weeks

**ðŸ”§ Operate Phase**
- **Approach**: Internal team can operate with external support for optimization
- **Critical**: Establish continuous improvement process

**âœ… Recommendation**: 
- **This is the sweet spot** for full methodology application
- Use standard timelines and phases
- Focus on scaling capability (move toward Level 4)

---

#### **Level 4 (Managed/Performing) - Accelerated Approach**

**ðŸŽ¯ Business Envisioning Workshop**
- **Duration**: Can compress to half-day if stakeholders are AI-fluent
- **Focus**: Strategic portfolio planning; OKR alignment
- **Deliverable**: AI portfolio roadmap with clear strategic themes

**ðŸ” Mobilisation Phase**
- **Duration**: 1-2 weeks (compressed)
- **Focus**: Team assembly and kickoff only

**ðŸ—ï¸ Build Phase**
- **Approach**: Internal team-led; external for cutting-edge AI only
- **Duration**: 2-3 months
- **Engagement Mode**: Advisory/coaching only (see Section 6.2)

**âœ… Recommendation**: 
- Use **fast-track 3-month delivery model** (Guide 93)
- Focus on complex/innovative use cases
- Share learnings with industry (thought leadership)

---

#### **Level 5 (Optimizing/Leading) - Innovation Approach**

**ðŸŽ¯ Business Envisioning Workshop**
- **May not need**: Organization already has strategic AI roadmap
- **If conducted**: Focus on cutting-edge capabilities (multi-agent, autonomous systems)

**Methodology Usage**:
- **Selective application**: Use specific guides as needed, not full methodology
- **Focus**: Innovation, research partnerships, thought leadership
- **Engagement**: Peer collaboration, not traditional consulting

**âœ… Recommendation**: 
- Contribute to this methodology! Share your patterns.
- Focus on AI-native business model transformation
- Mentor Level 2-3 organizations

---

### 6.2 Selecting Engagement Model Based on Maturity

Use these results to determine appropriate partner engagement from **Guide 92: Engagement Modes Framework**:

| Maturity Level | Recommended Engagement Mode | Rationale |
|----------------|----------------------------|-----------|
| **Level 1** | **Consulting + Academy** | Need education, strategy development, and capability building before delivery |
| **Level 2** | **Co-Delivery** or **Managed Service** | Need hands-on guidance and knowledge transfer during delivery |
| **Level 3** | **Staff Augmentation** or **Advisory** | Have core capability; need specific expertise and guidance |
| **Level 4** | **Advisory** or **Peer Collaboration** | Self-sufficient; need expertise for specific challenges only |
| **Level 5** | **Peer Collaboration** | Partners as equals for innovation initiatives |

**Specific Dimension Gaps:**
- **Strategy & Leadership <2.5**: Add Executive Coaching engagement
- **Data & Infrastructure <2.5**: Add Data Readiness Assessment + Platform Setup managed service
- **Technology & Operations <2.5**: Add MLOps Foundation Setup managed service
- **Talent & Culture <2.5**: Add Academy/Training program + Staff Augmentation
- **Governance & Ethics <2.5**: Add AI Governance Framework Setup consulting

---

## 7. Action Planning Based on Maturity Level

### 7.1 Level 1 â†’ Level 2 Improvement Plan (6-12 Months)

**Priority 1: Strategy & Leadership (Months 1-3)**
- [ ] Conduct Executive Coaching Workshop (Guide 90)
- [ ] Establish AI Vision and 3-year strategy
- [ ] Appoint Executive Sponsor with clear mandate
- [ ] Create AI Steering Committee (monthly meetings)
- [ ] Define success metrics and KPIs for AI program
- [ ] Secure multi-year budget commitment

**Priority 2: Data & Infrastructure (Months 2-6)**
- [ ] Complete Data Readiness Assessment
- [ ] Document and catalog critical data assets
- [ ] Establish data governance framework
- [ ] Setup Azure AI platform (Guide 03-setup-platform)
- [ ] Implement basic data quality processes
- [ ] Create data dictionary and metadata repository

**Priority 3: Talent & Culture (Months 3-9)**
- [ ] Assess current AI skills inventory
- [ ] Create upskilling program (Coursera/Udacity partnership)
- [ ] Hire 2-3 senior AI/ML engineers (external)
- [ ] Identify internal AI champions (5-10 people)
- [ ] Launch "AI 101" program for all employees
- [ ] Establish innovation time policy (10-20% time for experiments)

**Priority 4: Governance & Ethics (Months 4-8)**
- [ ] Establish AI Ethics Board (Guide 96)
- [ ] Create Responsible AI principles
- [ ] Define model risk management process
- [ ] Implement AI project approval gate
- [ ] Create AI incident response plan

**Priority 5: Technology & Operations (Months 6-12)**
- [ ] Complete first Hackathon/POC (Guide 02)
- [ ] Establish MLOps foundation (Guide 03)
- [ ] Setup CI/CD pipeline for ML
- [ ] Implement model monitoring basics
- [ ] Document ML operations playbook

**Success Metrics**:
- âœ“ Re-assess at Month 12; target overall score 2.2-2.6
- âœ“ All dimensions >2.0 (no dimension below 2.0)
- âœ“ Successfully completed 1-2 POCs with measurable learning
- âœ“ 50+ employees trained in AI fundamentals

---

### 7.2 Level 2 â†’ Level 3 Improvement Plan (9-15 Months)

**Priority 1: Technology & Operations (Months 1-6)**
- [ ] Deploy first production AI model (follow full methodology)
- [ ] Operationalize MLOps practices (automated retraining, monitoring)
- [ ] Establish model performance SLAs
- [ ] Implement A/B testing framework
- [ ] Create runbook for ML operations (Guide 08)

**Priority 2: Data & Infrastructure (Months 2-8)**
- [ ] Implement data quality automation
- [ ] Establish feature store
- [ ] Create self-service data platform
- [ ] Implement data lineage tracking
- [ ] Setup experiment tracking system (MLflow/Azure ML)

**Priority 3: Strategy & Leadership (Months 3-9)**
- [ ] Establish AI Center of Excellence (Guide 91)
- [ ] Define AI operating model
- [ ] Create AI capability roadmap
- [ ] Implement portfolio management for AI projects
- [ ] Establish AI investment review process

**Priority 4: Talent & Culture (Months 1-12)**
- [ ] Grow AI team to 10-15 people
- [ ] Establish AI community of practice (100+ members)
- [ ] Create internal AI certification program
- [ ] Launch "AI Champions" network across business units
- [ ] Implement AI-first mindset in strategic planning

**Priority 5: Governance & Ethics (Months 4-10)**
- [ ] Operationalize model risk management
- [ ] Implement model inventory and registry
- [ ] Establish model validation process (3-eyes review)
- [ ] Create AI explainability standards
- [ ] Conduct AI ethics training for all AI team members

**Success Metrics**:
- âœ“ Re-assess at Month 15; target overall score 3.0-3.4
- âœ“ All dimensions >2.7
- âœ“ 3-5 models in production with <5% incident rate
- âœ“ AI Center of Excellence established and operational

---

### 7.3 Level 3 â†’ Level 4 Improvement Plan (12-18 Months)

**Priority 1: Scale & Optimization**
- [ ] Grow to 10+ production models
- [ ] Establish model factory (standardized build process)
- [ ] Implement automated ML for appropriate use cases
- [ ] Create AI platform as internal product
- [ ] Establish AI innovation lab

**Priority 2: Strategic Integration**
- [ ] Integrate AI into business strategy (not separate)
- [ ] Establish AI metrics in executive scorecard
- [ ] Create AI business partnerships (vendors/startups)
- [ ] Develop AI-native products/services
- [ ] Lead industry working groups on AI

**Priority 3: Maturity Excellence**
- [ ] Publish internal AI playbook
- [ ] Achieve ISO/IEC 42001 AI Management System certification
- [ ] Establish AI research partnerships (universities)
- [ ] Create thought leadership content (whitepapers, conferences)
- [ ] Mentor Level 2-3 organizations

**Success Metrics**:
- âœ“ Re-assess at Month 18; target overall score 3.8-4.2
- âœ“ All dimensions >3.5
- âœ“ 10+ production models with <2% incident rate
- âœ“ Measurable competitive advantage from AI

---

### 7.4 Level 4 â†’ Level 5 Improvement Plan (18-24 Months)

**Focus**: Innovation leadership, not just operational excellence

**Priority 1: Innovation**
- [ ] Research and deploy cutting-edge AI (agents, multimodal, etc.)
- [ ] Establish AI research lab with PhD researchers
- [ ] File AI/ML patents
- [ ] Contribute to open-source AI projects
- [ ] Partner with AI research institutions

**Priority 2: Industry Leadership**
- [ ] Keynote at major industry conferences
- [ ] Publish research papers
- [ ] Contribute to AI standards bodies
- [ ] Create AI accelerator/incubator program
- [ ] Mentor startups and Level 2-3 organizations

**Priority 3: Business Transformation**
- [ ] AI-native business model components
- [ ] Monetize AI capabilities (AI-as-a-Service)
- [ ] Establish AI ecosystem partnerships
- [ ] Create AI-powered products as primary revenue stream

**Success Metrics**:
- âœ“ Re-assess at Month 24; target overall score 4.5-5.0
- âœ“ Industry recognition as AI leader
- âœ“ Measurable revenue from AI-powered products
- âœ“ Contributing to industry AI advancement

---

## 8. Using Results with Other Tools

### 8.1 Tool Selection Based on Maturity

The AI Delivery Methodology provides 7 interactive tools. Your maturity level determines which tools to use and in what order:

#### **All Organizations (Level 1-5): Start Here**

**ðŸŽ¯ [AI Maturity Assessment](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/ai-maturity-assessment.html)**
- **When**: Before any other tool
- **Purpose**: Establish baseline; determine which tools are appropriate
- **Output**: Maturity level and dimension scores

---

#### **Level 1 Organizations: Education First**

**ðŸ¤– [Value Analysis Chatbot](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/value-analysis-chatbot.html)**
- **When**: After maturity assessment
- **Purpose**: Learn about AI use cases in your industry; understand what's possible
- **How to Use**: 
  - Ask: "What AI use cases are common in [your industry]?"
  - Ask: "What maturity level is needed for [specific use case]?"
  - Ask: "What quick wins can a Level 1 organization pursue?"
- **Output**: Use case ideas and educational content

**â±ï¸ [Effort Estimator](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/effort-estimator.html)**
- **When**: After identifying 1-2 simple use cases from chatbot
- **Purpose**: Understand realistic timelines for your maturity level
- **How to Use**: 
  - Select "Low" complexity
  - Select your maturity level (Level 1)
  - Note: Timeline will be 2-3x longer than Level 3 org
- **Output**: Realistic timeline expectations

**âŒ Do NOT Use Yet:**
- ROI Calculator (too early; focus on learning, not ROI)
- Team Sizer (don't have team needs defined yet)
- Azure Cost Estimator (premature for infrastructure planning)

---

#### **Level 2 Organizations: Pilot Planning**

**Tool Sequence for Your First AI Project:**

**Step 1: ðŸ¤– [Value Analysis Chatbot](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/value-analysis-chatbot.html)**
- **Purpose**: Identify 3-5 candidate use cases
- **Ask**: "What AI use cases have high ROI and moderate complexity for [industry]?"
- **Ask**: "What data is typically needed for [specific use case]?"

**Step 2: â±ï¸ [Effort Estimator](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/effort-estimator.html)**
- **Purpose**: Estimate delivery timeline for each candidate use case
- **Input**: Use "Low" to "Medium" complexity; select Level 2 maturity
- **Compare**: Timeline for 3-5 use cases

**Step 3: ðŸ’° [ROI Calculator](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/roi-calculator.html)**
- **Purpose**: Calculate business case for top 2-3 use cases
- **Critical**: Be realistic with benefits (use conservative estimates)
- **Output**: ROI, payback period, NPV for each use case

**Step 4: ðŸ‘¥ [Team Sizer](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/team-sizer.html)**
- **Purpose**: Understand team requirements for selected use case
- **Input**: 
  - Select your maturity level (Level 2)
  - Select use case complexity from Effort Estimator
  - Note recommended team size and roles
- **Output**: Team composition, including external support needs

**Step 5: â˜ï¸ [Azure Cost Estimator](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/azure-cost-estimator.html)**
- **Purpose**: Estimate cloud infrastructure costs
- **Input**: Project size from Team Sizer
- **Output**: Monthly Azure spend estimate

**Final Step: Compile Business Case**
- Effort Estimator â†’ Timeline
- ROI Calculator â†’ Financial benefits
- Team Sizer â†’ Resource costs
- Azure Cost Estimator â†’ Infrastructure costs
- **Total Cost = Resource costs + Infrastructure costs**
- **Net Benefit = Financial benefits - Total Cost**

---

#### **Level 3 Organizations: Portfolio Planning**

**Tool Sequence for AI Portfolio Roadmap:**

**Step 1: ðŸ¤– [Value Analysis Chatbot](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/value-analysis-chatbot.html)**
- **Purpose**: Brainstorm 10-15 use cases across departments
- **Ask**: "What AI use cases deliver quick wins?" (for Year 1)
- **Ask**: "What AI use cases deliver transformational value?" (for Year 2-3)

**Step 2: Create Use Case Matrix (Spreadsheet)**
| Use Case | Complexity | Maturity Required | ROI | Timeline | Priority |
|----------|-----------|-------------------|-----|----------|----------|
| Use case 1 | Low | 2 | High | 3 months | P1 |
| Use case 2 | Medium | 3 | Medium | 6 months | P2 |
| ... | ... | ... | ... | ... | ... |

**Step 3: For Each High-Priority Use Case:**
- Run through Effort Estimator â†’ ROI Calculator â†’ Team Sizer â†’ Azure Cost Estimator
- Document in business case template

**Step 4: Portfolio Optimization**
- Balance quick wins (3-6 months) with strategic bets (9-18 months)
- Ensure team capacity isn't over-allocated (use Team Sizer totals)
- Sequence projects to build capability (start with lower complexity)

**Step 5: ðŸ‘¥ [Team Sizer](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/team-sizer.html) - Portfolio View**
- **Purpose**: Size the overall AI team for your 12-18 month roadmap
- **Input**: Multiple projects with staggered timelines
- **Output**: Core team size + project-specific augmentation needs

---

#### **Level 4-5 Organizations: Strategic Planning**

**Tool Usage:**
- Use tools selectively for specific analyses
- Focus on Value Analysis Chatbot for emerging AI capabilities research
- Use ROI Calculator for business model transformation scenarios
- Team Sizer for innovation lab / AI CoE sizing

**Advanced Usage:**
- Build your own tools based on these templates
- Integrate tool outputs into your portfolio management systems
- Automate scenario planning with API integrations (if available)

---

### 8.2 Maturity Assessment + Team Sizer Integration

The **Team Sizer** tool automatically adjusts recommendations based on maturity level. Here's how to use them together:

**Step 1: Complete Maturity Assessment**
- Note your overall maturity level (1-5)
- Note your Talent & Culture dimension score specifically

**Step 2: Open Team Sizer**
- Select your maturity level from dropdown
- The tool will automatically adjust:
  - Recommended team size (Level 1-2 need larger teams)
  - External support requirements (Level 1-2 need more)
  - Training needs (Level 1-2 need more upskilling)

**Step 3: Interpret Results**
- If Talent & Culture score <2.5: Add 20-30% more team size for knowledge transfer
- If Technology & Operations score <2.5: Ensure DevOps/MLOps resources are external hires
- If Strategy & Leadership score <2.5: Add dedicated Program Manager (senior, external)

---

### 8.3 Maturity Assessment + ROI Calculator Integration

**Adjust ROI Expectations Based on Maturity:**

| Maturity Level | ROI Multiplier | Rationale |
|----------------|----------------|-----------|
| **Level 1-2** | **0.5x - 0.7x** | Longer timelines, more rework, lower initial benefits |
| **Level 3** | **1.0x** | Baseline ROI as calculated |
| **Level 4** | **1.2x - 1.5x** | Faster delivery, better adoption, optimized operations |
| **Level 5** | **1.5x - 2.0x** | Industry-leading efficiency, innovation premium |

**How to Apply:**
1. Calculate ROI using ROI Calculator with standard assumptions
2. Apply maturity multiplier to benefits (not costs)
3. Example: 
   - Base ROI: 250% over 3 years
   - Maturity Level 2: 250% Ã— 0.6 = 150% adjusted ROI
   - Maturity Level 4: 250% Ã— 1.3 = 325% adjusted ROI

**Adjust Timeline Expectations:**
- Level 1-2: Add 50-100% to time-to-value from Effort Estimator
- Level 3: Use Effort Estimator results as-is
- Level 4-5: Can reduce timeline by 20-30%

---

## 9. Best Practices

### 9.1 Assessment Execution

âœ… **Do:**
- Complete assessment collaboratively with key stakeholders present
- Use evidence and examples to justify scores (not aspirations)
- Treat it as a baseline, not a performance review
- Re-assess every 6-12 months to track progress
- Share results transparently with project team
- Use results to inform project planning and risk management

âŒ **Don't:**
- Let one person (especially executive sponsor) complete alone
- Score aspirationally ("where we want to be" vs "where we are")
- Skip re-assessment after capability building initiatives
- Use results to blame or punish teams
- Ignore dimension-level gaps in favor of overall score
- Proceed with production AI if overall maturity <2.5

---

### 9.2 Results Communication

**To Executives:**
- Lead with: Overall maturity level, comparison to industry (12% at Level 4-5)
- Frame as: Current state baseline, NOT criticism
- Emphasize: Maturity predicts success rate (show statistics from Section 1.2)
- Present: Specific improvement roadmap with timelines
- Request: Budget and commitment for capability building

**To Project Team:**
- Lead with: Dimension scores and specific capability gaps
- Frame as: Opportunity to build skills and capabilities
- Emphasize: We're adjusting approach to match current maturity (not lowering expectations)
- Present: Training plans, support resources, realistic timelines
- Request: Patience and commitment to learning journey

**To Stakeholders:**
- Lead with: What this means for project timeline and approach
- Frame as: Risk mitigation through realistic planning
- Emphasize: Higher probability of success by matching approach to maturity
- Present: Adjusted roadmap with clear milestones
- Request: Support for capability building in addition to delivery

---

### 9.3 Continuous Improvement

**Quarterly Review Process:**
1. **Month 1**: Complete initial maturity assessment (baseline)
2. **Month 3**: Quick pulse check - focus on dimensions with improvement initiatives
3. **Month 6**: Full re-assessment with original stakeholder group
4. **Month 9**: Quick pulse check
5. **Month 12**: Full re-assessment + celebrate improvements

**Improvement Tracking:**
- Create maturity improvement dashboard
- Track dimension scores over time (line chart)
- Highlight initiatives that moved the needle
- Share wins and learnings with organization

**Governance Integration:**
- Add maturity review to AI Steering Committee agenda (quarterly)
- Gate decisions on new AI projects based on maturity improvements
- Tie AI CoE budget to demonstrated maturity gains

---

## 10. Common Pitfalls to Avoid

### 10.1 Assessment Pitfalls

âŒ **"We're More Mature Than We Are"**
- **Symptom**: Scoring Level 3-4 but no production AI models operating successfully
- **Reality Check**: If you can't point to specific evidence (documentation, systems, processes), you're probably 1 level lower
- **Fix**: Use evidence-based scoring; validate with technical team

âŒ **"One Person Knows Best"**
- **Symptom**: Executive or single leader completes assessment alone
- **Problem**: Misses reality of organizational capability; creates false baseline
- **Fix**: Require multi-stakeholder collaborative assessment

âŒ **"We'll Improve Naturally As We Deliver"**
- **Symptom**: Low maturity score, but plan to proceed with production AI anyway
- **Problem**: 70% chance of project failure; wasted investment
- **Fix**: Explicit capability building initiatives before/alongside delivery

---

### 10.2 Methodology Application Pitfalls

âŒ **"One Size Fits All"**
- **Symptom**: Applying standard methodology phases regardless of maturity level
- **Problem**: Level 1-2 orgs get overwhelmed; Level 4-5 orgs feel constrained
- **Fix**: Use maturity-adjusted methodology (Section 6.1)

âŒ **"Technology Will Solve It"**
- **Symptom**: Investing heavily in AI tools/platforms at Level 1-2 maturity
- **Problem**: "All tools, no skills" - tech stack collects dust
- **Fix**: Balance technology investment with talent development

âŒ **"We Can Skip Steps"**
- **Symptom**: Level 2 org wants to skip Mobilisation and go straight to Build
- **Problem**: Foundation gaps create major issues in Deploy/Operate phases
- **Fix**: Follow maturity-appropriate methodology phases; don't skip

---

### 10.3 Tool Usage Pitfalls

âŒ **"Tools Before Assessment"**
- **Symptom**: Using ROI Calculator, Team Sizer before completing maturity assessment
- **Problem**: Tools give unrealistic results without maturity context
- **Fix**: Always start with AI Maturity Assessment

âŒ **"Optimistic ROI at Low Maturity"**
- **Symptom**: Level 1-2 org using aggressive ROI assumptions
- **Problem**: Business case doesn't account for maturity-related delays and risks
- **Fix**: Apply maturity multipliers (Section 8.3)

âŒ **"Ignoring Dimension Gaps"**
- **Symptom**: Proceeding with standard team size despite Talent & Culture score of 1.5
- **Problem**: Team will be under-skilled for project needs
- **Fix**: Adjust Team Sizer outputs based on dimension-level gaps

---

## 11. Quick Reference

### Mandatory Assessment Touchpoints

| Phase | Timing | Participants | Purpose | Tool Output Used For |
|-------|--------|--------------|---------|---------------------|
| **Pre-Sales** | 1-2 weeks before Business Envisioning | Exec Sponsor, IT/Data Leaders | Baseline organizational readiness | Workshop tailoring, engagement model |
| **Mobilisation** | Week 1 of project | Full project team | Validate with technical perspective | Team structure, training plan, governance setup |
| **Discovery** | Week 2-3 | Capability area owners | Deep-dive weak dimensions | Capability build plans, quick wins |

### Recommended Assessment Touchpoints

| Phase | Timing | Participants | Purpose |
|-------|--------|--------------|---------|
| **Phase Gates** | Before Build, Deploy, Operate | Steering Committee | Verify readiness for next phase complexity |
| **Quarterly** | Every 3 months | Core team | Track improvement progress |
| **Post-Project** | 30-60 days after go-live | Full stakeholder group | Measure capability lift |

### Tool Usage Sequence by Maturity

**Level 1**: Maturity Assessment â†’ Value Chatbot â†’ Effort Estimator  
**Level 2**: Maturity Assessment â†’ Value Chatbot â†’ Effort Estimator â†’ ROI Calculator â†’ Team Sizer â†’ Azure Cost Estimator  
**Level 3-5**: All tools as needed for specific analyses

---

## 12. Appendix: Assessment Question Mapping to Methodology

### Strategy & Leadership Questions â†’ Methodology Impact

**Q1: AI Vision & Strategy**
- **If Level 1-2**: Extend Business Envisioning Workshop; add Executive Coaching
- **If Level 3+**: Standard workshop duration

**Q2: AI Governance**
- **If Level 1-2**: Create AI Steering Committee in Mobilisation phase
- **If Level 3+**: Leverage existing governance structures

**Q3: Executive Sponsorship**
- **If Level 1-2**: Weekly sponsor engagement; risk of disengagement
- **If Level 3+**: Standard monthly steering committee cadence

---

### Data & Infrastructure Questions â†’ Methodology Impact

**Q4: Data Accessibility**
- **If Level 1-2**: Add 2-4 week data readiness assessment to Discovery phase
- **If Level 3+**: Standard data discovery activities

**Q5: Data Quality**
- **If Level 1-2**: Add data quality improvement project before AI build
- **If Level 3+**: Standard data preparation in Build phase

**Q6: Infrastructure Readiness**
- **If Level 1-2**: Add 2-3 week platform setup phase (Guide 03)
- **If Level 3+**: Infrastructure ready; focus on ML-specific setup

---

### Technology & Operations Questions â†’ Methodology Impact

**Q7: AI/ML Capability**
- **If Level 1-2**: Start with Hackathon (Guide 02); delay production
- **If Level 3+**: Proceed to production build

**Q8: MLOps Maturity**
- **If Level 1-2**: Establish MLOps foundation as part of setup (Guide 03)
- **If Level 3+**: Leverage existing MLOps practices

**Q9: Model Operations**
- **If Level 1-2**: Managed service for initial operations (Guide 08)
- **If Level 3+**: Internal team can operate with advisory support

---

### Talent & Culture Questions â†’ Methodology Impact

**Q10: AI Skills**
- **If Level 1-2**: 50%+ staff augmentation; intensive training program
- **If Level 3+**: Selective staff augmentation for specific expertise

**Q11: Collaboration**
- **If Level 1-2**: Heavyweight change management; dedicated change lead
- **If Level 3+**: Standard stakeholder engagement

**Q12: Innovation Culture**
- **If Level 1-2**: Pilot project framed as "experiment"; lower stakes
- **If Level 3+**: Production deployment with appropriate risk management

---

### Governance & Ethics Questions â†’ Methodology Impact

**Q13: AI Ethics**
- **If Level 1-2**: Establish AI Ethics Board; create Responsible AI framework
- **If Level 3+**: Apply existing framework

**Q14: Model Risk Management**
- **If Level 1-2**: Implement basic model risk management (Guide 96)
- **If Level 3+**: Follow established model risk process

**Q15: Regulatory Compliance**
- **If Level 1-2**: Add compliance assessment; engage legal early
- **If Level 3+**: Standard compliance integration

---

## Summary

The **AI Maturity Assessment** is your roadmap to successful AI delivery. By honestly assessing your organization's current state, you can:

1. âœ… **Set realistic expectations** for timeline, cost, and outcomes
2. âœ… **Choose the right approach** - adjusted methodology for your maturity level
3. âœ… **Identify and close capability gaps** before they derail your project
4. âœ… **Select appropriate engagement models** for external support
5. âœ… **Use the right tools in the right order** for planning and execution
6. âœ… **Track improvement over time** and celebrate progress

**Remember**: There is no "good" or "bad" maturity level - only realistic self-awareness that enables better decisions. A Level 2 organization that knows it's Level 2 and plans accordingly will succeed. A Level 2 organization that thinks it's Level 4 will fail.

---

## Quick Links

- ðŸŽ¯ **[Take the Assessment Now](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/ai-maturity-assessment.html)**
- ðŸ¤– **[Value Analysis Chatbot](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/value-analysis-chatbot.html)**
- ðŸ‘¥ **[Team Sizer](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/team-sizer.html)**
- ðŸ’° **[ROI Calculator](https://andreaswasita.github.io/AI-Delivery-Methodology/calculators/roi-calculator.html)**
- ðŸ“š **[Full Methodology Guides](../guides/)**
- ðŸ¢ **[AI Center of Excellence Guide](./91-ai-center-of-excellence-framework.md)**
- ðŸŽ¯ **[Executive Coaching Guide](./90-executive-coaching-guide.md)**

---

**Document Version**: 1.0  
**Last Updated**: January 31, 2026  
**Maintained By**: AI Delivery Methodology Team
